{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GAN with MNIST.ipynb","provenance":[],"authorship_tag":"ABX9TyNmJt2SCJU7Sa2Y9CzSb1DP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"hCQgSlNLjWj4","executionInfo":{"status":"ok","timestamp":1621961555569,"user_tz":-120,"elapsed":2195,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["import numpy as np\n","import torch\n","import matplotlib.pyplot as plt"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"id":"dAWto8R1pTnR","executionInfo":{"status":"error","timestamp":1621998839008,"user_tz":-120,"elapsed":37283441,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}},"outputId":"2bec7d32-a95f-4ead-ea60-c7dfb711735d"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":3,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-4996ee3d8d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"jJAxF4C2pDQD","executionInfo":{"status":"aborted","timestamp":1621998837257,"user_tz":-120,"elapsed":1153,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["from torchvision import datasets\n","import torchvision.transforms as transforms\n","\n","# number of subprocesses to use for data loading\n","num_workers = 0\n","\n","batch_size = 64  # how many samples per batch to load\n","\n","transform = transforms.ToTensor()# convert data to torch.FloatTensor\n","\n","train_data = datasets.MNIST(root='/content/gdrive/My Drive/GAN/data', train=True,       # get the training datasets\n","                                   download=True, transform=transform)\n","\n","# prepare data loader\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n","                                           num_workers=num_workers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bJH4gdAfbVpQ","executionInfo":{"status":"aborted","timestamp":1621998837259,"user_tz":-120,"elapsed":1154,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["images, labels = next(iter(train_loader))\n","images.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y9FYBOPaqpqE","executionInfo":{"status":"aborted","timestamp":1621998837261,"user_tz":-120,"elapsed":1155,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["\n","# obtain one batch of training images\n","images, labels = next(iter(train_loader))\n","\n","img = images[0].view(28, 28) #img = images[0].numpy(), img = np.squeeze(img) #or use #img = transforms.ToPILImage(mode='L')(data)\n","plt.figure(figsize=[3, 3])\n","plt.imshow(img, cmap=\"gray\")\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AtVomgFrvPIt"},"source":["# Define the Model\n","A GAN is comprised of two adversarial networks, a discriminator and a generator.\n","\n","## Discriminator\n","**The discriminator network is going to be a pretty typical linear classifier.** \n","To make this network a universal function approximator, we'll need at least one hidden layer, and these hidden layers should have one key attribute:\n","\n","All hidden layers will have a Leaky ReLu activation function applied to their outputs."]},{"cell_type":"code","metadata":{"id":"t2sE-_62tBz5","executionInfo":{"status":"aborted","timestamp":1621998837262,"user_tz":-120,"elapsed":1156,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8V_lOksqlF4"},"source":[" \n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Discriminator(nn.Module):\n","\n","    def __init__(self, input_size, hidden_dim, output_size):\n","        super(Discriminator, self).__init__()\n","        \n","        # define hidden linear layers\n","        self.fc1 = nn.Linear(input_size, hidden_dim*4)\n","        self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)\n","        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\n","        \n","        # final fully-connected layer\n","        self.fc4 = nn.Linear(hidden_dim, output_size)\n","        \n","        # dropout layer \n","        self.dropout = nn.Dropout(0.3)\n","        \n","        \n","    def forward(self, x):\n","        # flatten image\n","        x = x.view(-1, 28*28)\n","        # all hidden layers\n","        x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n","        x = self.dropout(x)\n","        x = F.leaky_relu(self.fc2(x), 0.2)\n","        x = self.dropout(x)\n","        x = F.leaky_relu(self.fc3(x), 0.2)\n","        x = self.dropout(x)\n","        # final layer\n","        out = self.fc4(x)\n","\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YvFFuW56yq4P"},"source":["# Generator\n","The generator network will be almost exactly the same as the discriminator network, except that we're applying a tanh activation function to our output layer.\n","\n","## tanh Output\n","The generator has been found to perform the best with $tanh$ for the generator output, which scales the output to be between -1 and 1, instead of 0 and 1"]},{"cell_type":"code","metadata":{"id":"PnNep0ZmyqBC","executionInfo":{"status":"aborted","timestamp":1621998837262,"user_tz":-120,"elapsed":1156,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["class Generator(nn.Module):\n","\n","    def __init__(self, input_size, hidden_dim, output_size):\n","        super(Generator, self).__init__()\n","        \n","        # define hidden linear layers\n","        self.fc1 = nn.Linear(input_size, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim*2)\n","        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4)\n","        \n","        # final fully-connected layer\n","        self.fc4 = nn.Linear(hidden_dim*4, output_size)\n","        \n","        # dropout layer \n","        self.dropout = nn.Dropout(0.3)\n","\n","    def forward(self, x):\n","        # all hidden layers\n","        x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n","        x = self.dropout(x)\n","        x = F.leaky_relu(self.fc2(x), 0.2)\n","        x = self.dropout(x)\n","        x = F.leaky_relu(self.fc3(x), 0.2)\n","        x = self.dropout(x)\n","        # final layer with tanh applied\n","        out = F.tanh(self.fc4(x))\n","\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lMcp5DwayqJ-","executionInfo":{"status":"aborted","timestamp":1621998837263,"user_tz":-120,"elapsed":1157,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ok_FOYk67Lun"},"source":["# Model hyperparameters\n"]},{"cell_type":"code","metadata":{"id":"3PR26dQ06e5L","executionInfo":{"status":"aborted","timestamp":1621998837263,"user_tz":-120,"elapsed":1156,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["#discriminator hyperparameter\n","input_size=784# input image (28*28)\n","d_hidden_size=32 # Size of last hidden layer in the discriminator\n","\n","d_output_size=1# either 0(fake) or 1 (real)\n","\n","#generator  hyperparameter\n","\n","z_size=100## Size of latent vector to give to generator, it is a hyperparameter and it could be any number \n","g_hidden_size=32 # Size of First hidden layer in the generator \n","\n","g_output_size=784 # output generated image (28*28)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gVxdYDyk6gAI"},"source":["# Build the full network"]},{"cell_type":"code","metadata":{"id":"OHVpHzC46fAW","executionInfo":{"status":"aborted","timestamp":1621998837263,"user_tz":-120,"elapsed":1156,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["\n","# instantiate discriminator and generator\n","D = Discriminator(input_size, d_hidden_size, d_output_size)\n","G = Generator(z_size, g_hidden_size, g_output_size)\n","\n","# check that they are as you expect\n","print(D)\n","print()\n","print(G)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ixSt688tBCk6"},"source":["# Discriminator and Generator Losses\n","Now we need to calculate the losses.\n","\n","## Discriminator Losses\n","* For the discriminator, the total loss is the sum of the losses for real and fake images, d_loss = d_real_loss + d_fake_loss.\n","* Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to set up the losses to reflect that.\n","\n","The losses will by binary cross entropy loss with logits, which we can get with BCEWithLogitsLoss. This combines a sigmoid activation function and and binary cross entropy loss in one function.\n","\n","For the real images, we want D(real_images) = 1. That is, we want the discriminator to classify the the real images with a label = 1, indicating that these are real. To help the discriminator generalize better, **the labels are reduced a bit from 1.0 to 0.9.** \n","\n","For this, we'll use the parameter smooth; if True, then we should smooth our labels. \n","\n","In PyTorch, this looks like labels = torch.ones(size) * 0.9\n","\n","The discriminator loss for the fake data is similar. We want D(fake_images) = 0, where the fake images are the generator output, fake_images = G(z).\n","\n","# Generator Loss\n","The generator loss will look similar only with flipped labels. The generator's goal is to get D(fake_images) = 1. In this case, the labels are flipped to represent that the generator is trying to fool the discriminator into thinking that the images it generates (fakes) are real!"]},{"cell_type":"code","metadata":{"id":"bmYFAAMxAu5D","executionInfo":{"status":"aborted","timestamp":1621998837264,"user_tz":-120,"elapsed":1157,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["\n","# Calculate losses\n","def real_loss(D_out, smooth=False):\n","    # compare logits to real labels\n","    # smooth labels if smooth=True\n","    batch_size=D_out.size(0)#or D_out.shape[0]\n","    labels=torch.ones(batch_size)    \n","\n","    if smooth==True:\n","      labels=labels * 0.9\n","\n","    criterion = nn.BCEWithLogitsLoss() #beter in generalization\n","    loss = criterion(D_out.squeeze(),labels)\n","    return loss\n","\n","def fake_loss(D_out):\n","    # compare logits to fake labels\n","    batch_size=D_out.size(0)\n","    labels=torch.zeros(batch_size)\n","    criterion = nn.BCEWithLogitsLoss() #beter in generalization\n","    loss = criterion(D_out.squeeze(),labels)\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hdv3is7FVp2L"},"source":["# Optimizers\n","We want to update the generator and discriminator variables separately. So we'll define two separate Adam optimizers."]},{"cell_type":"code","metadata":{"id":"SgFHuYrKVp-_","executionInfo":{"status":"aborted","timestamp":1621998837264,"user_tz":-120,"elapsed":1157,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["import torch.optim as optim\n","\n","# learning rate for optimizers\n","lr = 0.002\n","\n","# Create optimizers for the discriminator and generator\n","d_optimizer = optim.Adam(D.parameters(),lr)\n","g_optimizer =optim.Adam(G.parameters(),lr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTksklq2Xee2"},"source":["# Training\n","Training will involve alternating between training the discriminator and the generator. We'll use our functions real_loss and fake_loss to help us calculate the discriminator losses in all of the following cases.\n","\n","## Discriminator training\n","* Compute the discriminator loss on real, training images\n","* Generate fake images\n","* Compute the discriminator loss on fake, generated images\n","* Add up real and fake loss\n","* Perform backpropagation + an optimization step to update the discriminator's weights\n","##Generator training\n","* Generate fake images\n","* Compute the discriminator loss on fake images, using flipped labels!\n","* Perform backpropagation + an optimization step to update the generator's weights\n","# Saving Samples\n","\n","As we train, we'll also print out some loss statistics and save some generated \"fake\" samples."]},{"cell_type":"code","metadata":{"id":"8SIV4S8mVt7I","executionInfo":{"status":"aborted","timestamp":1621998837265,"user_tz":-120,"elapsed":1158,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["import pickle as pkl\n","\n","# training hyperparams\n","num_epochs = 100\n","\n","# keep track of loss and generated, \"fake\" samples\n","samples = []\n","losses = []\n","\n","print_every = 400\n","\n","# Get some fixed data for sampling. These are images that are held\n","# constant throughout training, and allow us to inspect the model's performance\n","sample_size=16\n","fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n","fixed_z = torch.from_numpy(fixed_z).float()\n","\n","# train the network\n","D.train()\n","G.train()\n","for epoch in range(num_epochs):\n","    \n","    for batch_i, (real_images, _) in enumerate(train_loader):\n","                \n","        batch_size = real_images.size(0)\n","        \n","        ## Important rescaling step ## \n","        real_images = real_images*2 - 1  # rescale input images from [0,1) to [-1, 1)\n","        \n","        # ============================================\n","        #            TRAIN THE DISCRIMINATOR\n","        # ============================================\n","        \n","        d_optimizer.zero_grad()\n","        \n","        # 1. Train with real images\n","\n","        # Compute the discriminator losses on real images \n","        # smooth the real labels\n","        D_real = D(real_images)\n","        d_real_loss = real_loss(D_real, smooth=True)\n","        \n","        # 2. Train with fake images\n","        \n","        # Generate fake images\n","        # gradients don't have to flow during this step\n","        with torch.no_grad():\n","            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n","            z = torch.from_numpy(z).float()\n","            fake_images = G(z)\n","        \n","        # Compute the discriminator losses on fake images        \n","        D_fake = D(fake_images)\n","        d_fake_loss = fake_loss(D_fake)\n","        \n","        # add up loss and perform backprop\n","        d_loss = d_real_loss + d_fake_loss\n","        d_loss.backward()\n","        d_optimizer.step()\n","        \n","        \n","        # =========================================\n","        #            TRAIN THE GENERATOR\n","        # =========================================\n","        g_optimizer.zero_grad()\n","        \n","        # 1. Train with fake images and flipped labels\n","        \n","        # Generate fake images\n","        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n","        z = torch.from_numpy(z).float()\n","        fake_images = G(z)\n","        \n","        # Compute the discriminator losses on fake images \n","        # using flipped labels!\n","        D_fake = D(fake_images)\n","        g_loss = real_loss(D_fake) # use real loss to flip labels\n","        \n","        # perform backprop\n","        g_loss.backward()\n","        g_optimizer.step()\n","\n","        # Print some loss stats\n","        if batch_i % print_every == 0:\n","            # print discriminator and generator loss\n","            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n","                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n","\n","    \n","    ## AFTER EACH EPOCH##\n","    # append discriminator loss and generator loss\n","    losses.append((d_loss.item(), g_loss.item()))\n","    \n","    # generate and save sample, fake images\n","    G.eval() # eval mode for generating samples\n","    samples_z = G(fixed_z)\n","    samples.append(samples_z)\n","    G.train() # back to train mode\n","\n","\n","# Save training generator samples\n","with open('train_samples.pkl', 'wb') as f:\n","    pkl.dump(samples, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ojvzLdtdy1X9"},"source":["# Training loss\n","Here we'll plot the training losses for the generator and discriminator, recorded after each epoch."]},{"cell_type":"code","metadata":{"id":"mLUbTuFGy1ik","executionInfo":{"status":"aborted","timestamp":1621998837265,"user_tz":-120,"elapsed":1157,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["fig, ax = plt.subplots()\n","losses = np.array(losses)\n","plt.plot(losses.T[0], label='Discriminator')\n","plt.plot(losses.T[1], label='Generator')\n","plt.title(\"Training Losses\")\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6nBmwK5p-MP-"},"source":["# Test our generator"]},{"cell_type":"code","metadata":{"id":"OmzIjGU_86Lb","executionInfo":{"status":"aborted","timestamp":1621998837266,"user_tz":-120,"elapsed":1158,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["z = np.random.uniform(-1, 1, size=(1, z_size))\n","z = torch.from_numpy(z).float()\n","fake_images = G(z)\n","print(fake_images.shape)\n","img = fake_images.view(28, 28)\n","img = img.detach()\n","plt.imshow(img, cmap=\"gray\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Vkg7n_pzfrA","executionInfo":{"status":"aborted","timestamp":1621998837266,"user_tz":-120,"elapsed":1158,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["\n","# helper function for viewing a list of passed in sample images\n","def view_samples(epoch, samples):\n","    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n","    for ax, img in zip(axes.flatten(), samples[epoch]):\n","        img = img.detach()\n","        ax.xaxis.set_visible(False)\n","        ax.yaxis.set_visible(False)\n","        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B-wTpMuxzftf","executionInfo":{"status":"aborted","timestamp":1621998837266,"user_tz":-120,"elapsed":1158,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["# Load samples from generator, taken while training\n","with open('train_samples.pkl', 'rb') as f:\n","    samples = pkl.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y1CugiVFzlte","executionInfo":{"status":"aborted","timestamp":1621998837267,"user_tz":-120,"elapsed":1158,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["# -1 indicates final epoch's samples (the last in the list)\n","view_samples(-1, samples)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mMlX7-azlww","executionInfo":{"status":"aborted","timestamp":1621998837267,"user_tz":-120,"elapsed":1157,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["rows = 10 # split epochs into 10, so 100/10 = every 10 epochs\n","cols = 6\n","fig, axes = plt.subplots(figsize=(7,12), nrows=rows, ncols=cols, sharex=True, sharey=True)\n","\n","for sample, ax_row in zip(samples[::int(len(samples)/rows)], axes):\n","    for img, ax in zip(sample[::int(len(sample)/cols)], ax_row):\n","        img = img.detach()\n","        ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n","        ax.xaxis.set_visible(False)\n","        ax.yaxis.set_visible(False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mA_mJxZGzsd0","executionInfo":{"status":"aborted","timestamp":1621998837268,"user_tz":-120,"elapsed":944,"user":{"displayName":"sana sabah","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf-H9Ctxtga8Ft4JJ1d6-qWV1dzJ0_wfPWKDFivQ=s64","userId":"08967291600381232529"}}},"source":["\n","# randomly generated, new latent vectors\n","sample_size=16\n","rand_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n","rand_z = torch.from_numpy(rand_z).float()\n","\n","G.eval() # eval mode\n","# generated samples\n","rand_images = G(rand_z)\n","\n","# 0 indicates the first set of samples in the passed in list\n","# and we only have one batch of samples, here\n","view_samples(0, [rand_images])"],"execution_count":null,"outputs":[]}]}